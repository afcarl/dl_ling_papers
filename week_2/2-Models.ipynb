{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Mitchell and Lapata (2010) define a general class of composition models defined as\n",
    "$$ \\mathbf{p} = \\text{comp}(u, v, R, K) $$\n",
    "representing the composition $\\mathbf{p}$ of two constituents $\\mathbf{u}, \\mathbf{v}$  which stand in some syntactic relation $R$, where $K$ represents any other information used. \n",
    "\n",
    "The models we describe here do not make use of the syntactic relation argument $R$, so let\n",
    "$$ \\text{comp}(u, v, K) = \\text{comp}(u, v, -, K) $$\n",
    "\n",
    "Two simplistic models which leave $R$ empty are the *additive model*\n",
    "$$ \\text{comp}(\\mathbf{u}, \\mathbf{v}, \\langle \\mathbf{A}, \\mathbf{B}\\rangle) = \\mathbf{uA} + \\mathbf{vB} $$\n",
    "and the *multiplicative model*\n",
    "$$ \\text{comp}(\\mathbf{u}, \\mathbf{v},  \\mathbf{C}) = (\\mathbf{u}\\odot\\mathbf{v})\\mathbf{C} $$\n",
    "\n",
    "where $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^d, \\mathbf{A, B, C} \\in \\mathbb{R}^{d \\times d}$. In a neural network model using these composition functions, the matrices $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ are trained and the word vectors $\\mathbf{u}, \\mathbf{v}$ may be given (e.g., by `word2vec`) or may also be trained or tuned by the model in parallel.\n",
    "\n",
    "Here I discuss two classes of models using Mitchell and Lapata's notation:\n",
    "1. Recursive neural models which assume trees over vector representations of words\n",
    "2. Models which represent certain words as matrices\n",
    "\n",
    "## Neural models with explicit compositional structure\n",
    "\n",
    "These models assume vector representations of each word in an input sequence, and use RNNs and variants of RNNs to learn composition functions on vectors. Depending on the task, the vector representations of the words may be learned or tuned by the model in parallel.\n",
    "\n",
    "### Recursive Neural Network (RNN)\n",
    "\n",
    "An input sequence $\\mathbf{x}$ of vectors (representing a sequence of words) is parsed into a binary tree structure.  Given two vectors $\\mathbf{a}, \\mathbf{b}$, the vector of the parent node $\\mathbf{p}$ is the output of the composition function\n",
    "$$ \\mathbf{p} = \\text{comp}(\\mathbf{a}, \\mathbf{b}, K). $$\n",
    "\n",
    "Socher et al. (2010) give this composition function:\n",
    "$$ \\text{comp}(\\mathbf{a}, \\mathbf{b}, \\mathbf{W}) = f\\left(\\begin{bmatrix}\\mathbf{a}\\\\\\mathbf{b}\\end{bmatrix}\\mathbf{W}\\right) $$\n",
    "where $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^d, \\mathbf{W} \\in \\mathbb{R}^{2d\\times d}$. $\\mathbf{W}$ is the parameter to learn and is fixed for all compositions of $\\mathbf{a}$ and $\\mathbf{b}$. $f$ is the activation function, usually $\\tanh$. The model may also include a bias term, not shown above.\n",
    "\n",
    "<img src=\"images/socher2010fig1.png\" />\n",
    "\n",
    "For a classification task, the softmax classifier is applied either to the root node or to each node $\\mathbf{a}$ in the tree (dependending on the structure of your training data)\n",
    "$$ \\mathbf{y^{(a)}} = \\text{softmax}(\\mathbf{a}\\mathbf{W}_s) $$\n",
    "where $\\mathbf{W}_s \\in \\mathbb{R}^{d\\times m}$ for an $m$-way classification task.\n",
    "\n",
    "### Matrix-Vector RNN (MV-RNN)\n",
    "\n",
    "An extension of an RNN, so that there can be a different composition function for each pair of children. Given two vectors $\\mathbf{a}, \\mathbf{b}$ they are associated respectively with the matrices $\\mathbf{A}, \\mathbf{B}$. So each node is represented by a pair of a vector and a matrix.\n",
    "\n",
    "<img src=\"images/socher2012fig2.png\" />\n",
    "\n",
    "The composition function is\n",
    "$$ \\text{comp}(\\langle\\mathbf{a}, \\mathbf{A}\\rangle, \\langle\\mathbf{b}, \\mathbf{B}\\rangle,  \\mathbf{W}) = \n",
    "\\left\\langle f\\left(\\begin{bmatrix}\\mathbf{Ba}\\\\\\mathbf{Ab}\\end{bmatrix}\\mathbf{W}\\right),\n",
    "\\begin{bmatrix}\\mathbf{A}\\\\\\mathbf{B}\\end{bmatrix}\\mathbf{W}_M \\right\\rangle$$\n",
    "where $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^d, \\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{d \\times d}, \\mathbf{W} \\in \\mathbb{R}^{2d\\times d}$, and $\\mathbf{W}_M \\in \\mathbb{R}^{2d\\times  d}$.\n",
    "\n",
    "Now, the parameters to learn are $\\mathbf{W}, \\mathbf{W}_M$ and the matrices $\\mathbf{X}$ for all leaf vectors $\\mathbf{x}$.\n",
    "\n",
    "### Other models\n",
    "\n",
    "- RNTN (Recursive Neural Tensor Network) (Socher et al. 2013): similar to MV-RNNs, but with a fixed number of parameters to learn.\n",
    "- FCN (Forest Convolutional Network) (Le & Zuidema 2015): extends RNN to sets of trees.\n",
    "- Tree LSTM (Tai, Socher, & Manning 2015): inspired by LSTMs and RNNs. Uses an LSTM-like architecture over trees, rather than sequences.\n",
    "\n",
    "\n",
    "## Matrix representations of predicates\n",
    "\n",
    "Taking inspiration from Montague semantics, these models represent relational terms (verbs, adjectives) as linear functions on vectors, encoded as matrices. I discuss two models that apply this idea to a restricted set of syntactic structures.\n",
    "\n",
    "### Adjective-noun pairs (Baroni & Zamparelli 2010)\n",
    "\n",
    "In an adjective-noun sequence (e.g., 'green chair'), represent the adjective as a matrix and the noun as a vector. The adjective acts as a linear function of nouns, so the composition function is defined as such:\n",
    "$$ \\text{comp}(\\mathbf{A}, \\mathbf{n}, -) = \\mathbf{nA} $$\n",
    "where $\\mathbf{n} \\in \\mathbb{R}^d, \\mathbf{A} \\in \\mathbb{R}^{d \\times d}$\n",
    "\n",
    "Baroni & Zamparelli (2010) assume the noun vectors are given, and learn adjective vectors. To do so, they treat each horizontal slice of $\\mathbf{A}$ as a linear regression model:\n",
    "$$ y_i = \\mathbf{A}_{[i,\\cdot]}x. $$\n",
    "So each adjective matrix $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$ is estimated by $d$ linear regressions.\n",
    "\n",
    "For each adjective, the regression model uses as training data a distributional adjective matrix constructed as follows:\n",
    "1. obtain a cooccurance matrix for the adjective\n",
    "2. transform the raw counts to Local Mutual Information (LMI) scores\n",
    "3. apply Singular Value Decomposition (SVD) to reduce dimensionality\n",
    "\n",
    "\n",
    "### Transitive verbs (Grefenstette & Sadrzadeh 2010, 2011)\n",
    "\n",
    "In a transitive sentence, represent the subject and object as vectors, and the verb as a matrix. The meaning of the whole sentence is the component-wise product of verb with the outer product of the subject and object.\n",
    "\n",
    "$$ \\overrightarrow{sub~verb~obj} = \\underline{verb} \\odot (\\overrightarrow{sub} \\otimes \\overrightarrow{obj}) $$\n",
    "\n",
    "where the outer product of two vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^d$ is:\n",
    "$$ [\\mathbf{u} \\otimes \\mathbf{v}]_{[i,j]} := u_iv_j. $$\n",
    "This gives a $d\\times d$ matrix\n",
    "$$ \\mathbf{u} \\otimes \\mathbf{v} = \\begin{bmatrix}\n",
    "    u_1v_1 & \\cdots & u_1v_d \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    u_dv_1 & \\cdots & u_dv_d\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can think of this as two separate composition functions, one between vectors which transforms them into matrices, and one between matrices:\n",
    "\n",
    "$$ \\text{comp}(\\mathbf{u}, \\mathbf{v}, -) = \\mathbf{u} \\otimes \\mathbf{v} $$\n",
    "$$ \\text{comp}(\\mathbf{A}, \\mathbf{B}, -) = \\mathbf{A} \\odot \\mathbf{B} $$\n",
    "\n",
    "So the meaning of the sentence is computed by\n",
    "$$ \\text{comp}(\\overrightarrow{verb}, \\text{comp}(\\overrightarrow{sub}, \\overrightarrow{obj}, -), -). $$\n",
    "\n",
    "Grefenstette & Sadrzadeh do not learn verb matrices, but rather experiment with three ways of generating verb matrices from verb vectors. Let $\\mathbf{v}$ be a verb vector. Consider defining the matrix $\\mathbf{V}$ in three ways:\n",
    "1. Embed $\\mathbf{v}$ as the diagonal of $\\mathbf{V}$, with all other entries set to 0.\n",
    "$$ \\mathbf{V} = \\begin{bmatrix}\n",
    "    v_1 & 0 & 0 & 0 \\\\\n",
    "    0 & v_2 & 0 & 0 \\\\\n",
    "    0 & 0 & \\ddots & 0 \\\\\n",
    "    0 & 0 & 0 & v_d \\\\\n",
    "\\end{bmatrix} $$\n",
    "2. Embed $\\mathbf{v}$ as the diagonal of $\\mathbf{V}$, with all other entries set to 1.\n",
    "$$ \\mathbf{V} = \\begin{bmatrix}\n",
    "    v_1 & 1 & 1 & 1 \\\\\n",
    "    1 & v_2 & 1 & 1 \\\\\n",
    "    1 & 1 & \\ddots & 1 \\\\\n",
    "    1 & 1 & 1 & v_d \\\\\n",
    "\\end{bmatrix} $$\n",
    "3. Take the outer product of the verb with itself.\n",
    "$$ \\mathbf{V} = \\mathbf{v} \\otimes \\mathbf{v} = \\begin{bmatrix}\n",
    "    v_1v_1 & \\cdots & v_1v_d \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    v_dv_1 & \\cdots & v_dv_d\n",
    "\\end{bmatrix} $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
