{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of models\n",
    "## I. Basic tools \n",
    "\n",
    "### Word order in Phrase Similarity \n",
    "* A data set of pairs of N-N, V-N and A-N phrases rated by humans with respect to similarity, e.g., \"certain circumstance\" and \"particular case\"  \n",
    "\n",
    "\n",
    "* Mitchell and Lapata (2010) and Turney (2012) obtained an extended version including word order variations of the original phrases, which are automatically judged to have a very low score of similarity, e.g., \"certain circumstance\" and \"case particular\"\n",
    "\n",
    "### Sentence similarity: Intransitive Sentences\n",
    "* Kintsch (2011) first looked at verb-argument composition, such as capturing the various verb senses generated by different argument structures, e.g., “The colour ran” vs.  “The horse ran”. \n",
    "\n",
    "\n",
    "* Mitchell and Lapata (2008) developed a larger data set of \"subject + intransitive-verb\" sentences. \n",
    "\n",
    "\n",
    "* Frequent N-V tuples, e.g., dog ran, were extracted from the British National Corpus and paired with sentences with 2 synonyms of the verb, in an attempt to represent distinct verb senses.\n",
    "\n",
    "\n",
    "* One of the verbs are compatible and the other uncompatible with the argument (e.g., dog galloped and dog dissolved). These tuples generated were then converted into sentences construced in the Simple Past Tense, with the addition of articles when appropriate, to reflect English Grammar.\n",
    "\n",
    "### Sentence Similarity: Transitive Sentences\n",
    "* Grefenstette and  Sadrzadeh  (2011)  developed  an  analogous  data  set  of  transitive  sentences. \n",
    "\n",
    "\n",
    "* The focus is on how arguments could influence the selection of the meaning of an ambiguous verb head. As an example, \"meet\" is an synonym of \"satisty\" and \"visit\". \n",
    "\n",
    "\n",
    "* Each verb was paired with a \"subject + transitive-verb + object\" tuple from the BNC, generating again sentences in the Simple Past. \n",
    "\n",
    "* Take \"met\" as an example, the two sentences “The system met the criterion” and “The child met the house” were produced. \n",
    "\n",
    "\n",
    "* For each sentence, two novel versions were generated by replacing the verb with two synonyms representing two verb senses, e.g., “The system visited the criterion” and “The system satisfied the criterion”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Pham et al. (2013)\n",
    "\n",
    "### Data Set\n",
    "* A data set of transitive and intransitive sentences that fall within the language recognized by the Context Free Grammar defined, as below:\n",
    "\n",
    "<img src=\"images/pham_eval_new.png\" />\n",
    "\n",
    "* A total of 32 verbs, 7 determiners, 65 nouns and 19 adjectives were used. The data is split into paraphrase set and foil sets. The paraphrase set indicates that two sentences can descibe essentially the same sitaution or context. \n",
    "\n",
    "\n",
    "* The foil set, focusing on discrptive word order and determiner chagnes, has the purpose to help spotting whether a DSM is \"cheating\" in accomplishing this task, or, better, if it does detect paraphrases but does nto properly catpure compostionality. \n",
    "\n",
    "\n",
    "* They collected co-occurence statistics fromt he concatenation of the BNC and English Wikipedia. They extracted distributional vectos for the 20K most frequent inflected nouns in the corpus, and all the verbs, adjetives and determiners in the vocabulary in their lemma forms. \n",
    "\n",
    "\n",
    "* A bag-of-word approach was adopted, counting co-occurence with all context words in the same sentence with a target item. \n",
    "\n",
    "### Sentence similarity to paraphrases vs. foils\n",
    "\n",
    "* Goal of the task: To highlight when the possible good performance in the paraphrase detection task does correspond to true modelling of compositionality; to measure the similarity of each sentence in the paraphrase set to all other sentences in the same group (i.e., valid paraphrases, P), as well as to sentences in the corresponding foil set (FP). \n",
    "\n",
    "\n",
    "* For each sentence in a P group, the mean of the cosine of the sentence with all the sentences in the same ground-truth group (with all the P sentences) $cos.para$ and the mean of the cosine with all the foil paraphrases (with all the FP sentences, viz. those marked by S, D, SD) of the same group $cos.foil$ were computed. \n",
    "\n",
    "\n",
    "* The difference is measured by $diff.para.foil = cos.para - cos.foil$. \n",
    "\n",
    "\n",
    "* Finally, the mean of diff.para.foil for all the sentences in the data set was calculated. \n",
    "\n",
    "\n",
    "* Intuitively, models which achieve higher values are those that are not captured by the foils’ trap. \n",
    "\n",
    "\n",
    "* The authors suggested that 'only a model that realizes that \"A man plays an instrument\" is a better paraphrase of \"A man plays guitar\" than either \"A guitar plays a man\" or \"The man plays no guitar\" can be said to truly catch compositional meaning, beyond simple word meaning overlap'. \n",
    "\n",
    "<img src=\"images/pham_eval_new.png\" />\n",
    "\n",
    "### Implications and results\n",
    "\n",
    "* Most models fell into the foils’ trap. \n",
    "\n",
    "\n",
    "* For neither multiplicative models the difference between similarity to true paraphrases vs. foils is signiﬁcantly above zero (here and below, statistical signiﬁcance measured by two-tailed t-tests). \n",
    "\n",
    "\n",
    "* The authors further suggested the following: 'Among additive models, only those that do include information about determiners have differences between paraphrase and foil similarity signiﬁcantly above zero. Indeed, since the additive model is by construction insensitive to word order, the fact that it displays a signiﬁcant difference at all indicates that evidently the vectors representing determiners are more informative about their meaning than we thought'. \n",
    "\n",
    "\n",
    "* As a side note, it is important not to neglect the fact that the only determiner replacement tested is the one from the positive determiners – a, some, the, one, two, three – to no. Clearly, more studies would have to be done in order to draw any sensible conclusions. \n",
    "\n",
    "\n",
    "* Although beyond the scope of today's discussion, the comparison of the values obtained for diff.para.foil and diff.para.scrambled is only interesting for the lexfunc models. The latter comparison tells us which models fail to compose meaning because they are insensitive to word order, whereas the former also takes determiners into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Blacoe & Lapata (2012)\n",
    "\n",
    "* They composed word vectors to create representations for phrase vectors and sentence vectors. \n",
    "\n",
    "\n",
    "* Phrases that were captured were 2-word pairs, such as: an adjective and a noun like black hair, a compound noun made up of two nouns such as oil industry, or a verbal phrase with a transitive verb and an object noun, e.g., pour tea.\n",
    "\n",
    "\n",
    "* Conceiving of a phrase phr = (w1,w2) as a binary tuple of words, we obtain its vector from its words’ vectors by addition:\n",
    "\n",
    "$$\n",
    "phrVec({w1},{w2}) = wdVec_{w1} + wdVec_{w2} \n",
    "$$\n",
    "\n",
    "* They also repeated it using point-wise multiplication:\n",
    "\n",
    "$$\n",
    "phrVec(w1,w2) = wdVec_{w1} \\odot wdVec_{w2}\n",
    "$$\n",
    "\n",
    "### The Experiment: Phrase Similarity\n",
    "\n",
    "* Focused on modeling similarity judgments for short phrases gathered in human experiments. \n",
    "\n",
    "\n",
    "* The evaluatation of the distributional representations of individual words are usually done based on their ability to capture semantic simiarlity relations, e.g. synonymy and priming. \n",
    "\n",
    "\n",
    "* The dataset from Mitchell and Lapata (2010), which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively, was used. \n",
    "\n",
    "\n",
    "* Each item is a phrase pair phr$_1$, phr$_2$ which has a human rating from 1 (very low similarity) to 7 (very high similarity). \n",
    "\n",
    "\n",
    "* Model similarities were evaluated against the human similarity ratings using Spearman’s r correlation coefficient.\n",
    "\n",
    "$$\n",
    "phrSIM_{phr1, phr2} = \\frac{phrVec_{phr1}  \\cdot  phrVec_{phr2}}{|phrVec_{phr1}| \\times |phrVec_{phr2}|}\n",
    "$$\n",
    "\n",
    "\n",
    "### Implications and Results\n",
    "\n",
    "* The table below summarizes the performance of the various models on the phrase similarity dataset. Correlation coefficients of model predictions with subject similarity ratings (Spearman’s r); columns show dimensionality, composition method: + is additive vector composition, $\\odot$ is component-wise multiplicative vector composition, RAE is Socher et al. (2011)’s recursive auto-encoder.\n",
    "\n",
    "<img src=\"images/blacoe_eval_new.png\" />\n",
    "\n",
    "* The rows in the table correspond to different vector representations: the simple distributional semantic space (SDS) from Mitchell and Lapata (2010), Baroni and Lenci’s (2010) distributional memory tensor (DM) and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), noun-noun (N-N) and verb object (V-Obj). \n",
    "\n",
    "\n",
    "* For each phrase type, the authors reported results for each compositional model, i.e. additive (+), multiplicative ($\\odot$) and recursive autoencoder (RAE). \n",
    "\n",
    "\n",
    "* Clearly the best performing model is multiplication, as it is mostly for DM. \n",
    "\n",
    "\n",
    "* With regard to NLM, vector addition yields overall better results. \n",
    "\n",
    "\n",
    "* In general, neither DM or NLM in any compositional conﬁguration are able to outperform SDS with multiplication.\n",
    "\n",
    "\n",
    "* All models in the table are signiﬁcantly correlated with the human similarity judg-ments (p < 0.01). Spearman’s r differences of 0.3 or more are signiﬁcant at the 0.01 level, using a t-test (Cohen and Cohen, 1983)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
